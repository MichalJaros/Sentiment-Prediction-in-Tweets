{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e22dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8e62f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22a87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce198e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>Mon May 11 03:17:40 UTC 2009</th>\n",
       "      <th>kindle2</th>\n",
       "      <th>tpryan</th>\n",
       "      <th>@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>Mon May 11 03:22:00 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>GeorgeVHulme</td>\n",
       "      <td>@richardebaker no. it is too big. I'm quite ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>2</td>\n",
       "      <td>14072</td>\n",
       "      <td>Sun Jun 14 04:31:43 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>proggit</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0</td>\n",
       "      <td>14073</td>\n",
       "      <td>Sun Jun 14 04:32:17 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>sam33r</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>4</td>\n",
       "      <td>14074</td>\n",
       "      <td>Sun Jun 14 04:36:34 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>iamtheonlyjosie</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0</td>\n",
       "      <td>14075</td>\n",
       "      <td>Sun Jun 14 21:36:07 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>plutopup7</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>14076</td>\n",
       "      <td>Sun Jun 14 21:36:17 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>captain_pete</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     4      3  Mon May 11 03:17:40 UTC 2009  kindle2           tpryan  \\\n",
       "0    4      4  Mon May 11 03:18:03 UTC 2009  kindle2           vcu451   \n",
       "1    4      5  Mon May 11 03:18:54 UTC 2009  kindle2           chadfu   \n",
       "2    4      6  Mon May 11 03:19:04 UTC 2009  kindle2            SIX15   \n",
       "3    4      7  Mon May 11 03:21:41 UTC 2009  kindle2         yamarama   \n",
       "4    4      8  Mon May 11 03:22:00 UTC 2009  kindle2     GeorgeVHulme   \n",
       "..  ..    ...                           ...      ...              ...   \n",
       "492  2  14072  Sun Jun 14 04:31:43 UTC 2009    latex          proggit   \n",
       "493  0  14073  Sun Jun 14 04:32:17 UTC 2009    latex           sam33r   \n",
       "494  4  14074  Sun Jun 14 04:36:34 UTC 2009    latex  iamtheonlyjosie   \n",
       "495  0  14075  Sun Jun 14 21:36:07 UTC 2009     iran        plutopup7   \n",
       "496  0  14076  Sun Jun 14 21:36:17 UTC 2009     iran     captain_pete   \n",
       "\n",
       "    @stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.  \n",
       "0    Reading my kindle2...  Love it... Lee childs i...                                                               \n",
       "1    Ok, first assesment of the #kindle2 ...it fuck...                                                               \n",
       "2    @kenburbary You'll love your Kindle2. I've had...                                                               \n",
       "3    @mikefish  Fair enough. But i have the Kindle2...                                                               \n",
       "4    @richardebaker no. it is too big. I'm quite ha...                                                               \n",
       "..                                                 ...                                                               \n",
       "492  Ask Programming: LaTeX or InDesign?: submitted...                                                               \n",
       "493  On that note, I hate Word. I hate Pages. I hat...                                                               \n",
       "494  Ahhh... back in a *real* text editing environm...                                                               \n",
       "495  Trouble in Iran, I see. Hmm. Iran. Iran so far...                                                               \n",
       "496  Reading the tweets coming out of Iran... The w...                                                               \n",
       "\n",
       "[497 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02178951",
   "metadata": {},
   "source": [
    "The received source data does not have headers. The next step will be to add them using the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb39783",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Tweets.csv', header=None)\n",
    "header = ['Sentiment', 'The id of the tweet', 'The date of the tweet', 'The query', 'The user that tweeted', 'The text of the tweet']\n",
    "dataset.columns = header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3afc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>The id of the tweet</th>\n",
       "      <th>The date of the tweet</th>\n",
       "      <th>The query</th>\n",
       "      <th>The user that tweeted</th>\n",
       "      <th>The text of the tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2</td>\n",
       "      <td>14072</td>\n",
       "      <td>Sun Jun 14 04:31:43 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>proggit</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>14073</td>\n",
       "      <td>Sun Jun 14 04:32:17 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>sam33r</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>14074</td>\n",
       "      <td>Sun Jun 14 04:36:34 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>iamtheonlyjosie</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>14075</td>\n",
       "      <td>Sun Jun 14 21:36:07 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>plutopup7</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>14076</td>\n",
       "      <td>Sun Jun 14 21:36:17 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>captain_pete</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment  The id of the tweet         The date of the tweet The query  \\\n",
       "0            4                    3  Mon May 11 03:17:40 UTC 2009   kindle2   \n",
       "1            4                    4  Mon May 11 03:18:03 UTC 2009   kindle2   \n",
       "2            4                    5  Mon May 11 03:18:54 UTC 2009   kindle2   \n",
       "3            4                    6  Mon May 11 03:19:04 UTC 2009   kindle2   \n",
       "4            4                    7  Mon May 11 03:21:41 UTC 2009   kindle2   \n",
       "..         ...                  ...                           ...       ...   \n",
       "493          2                14072  Sun Jun 14 04:31:43 UTC 2009     latex   \n",
       "494          0                14073  Sun Jun 14 04:32:17 UTC 2009     latex   \n",
       "495          4                14074  Sun Jun 14 04:36:34 UTC 2009     latex   \n",
       "496          0                14075  Sun Jun 14 21:36:07 UTC 2009      iran   \n",
       "497          0                14076  Sun Jun 14 21:36:17 UTC 2009      iran   \n",
       "\n",
       "    The user that tweeted                              The text of the tweet  \n",
       "0                  tpryan  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1                  vcu451  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2                  chadfu  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3                   SIX15  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4                yamarama  @mikefish  Fair enough. But i have the Kindle2...  \n",
       "..                    ...                                                ...  \n",
       "493               proggit  Ask Programming: LaTeX or InDesign?: submitted...  \n",
       "494                sam33r  On that note, I hate Word. I hate Pages. I hat...  \n",
       "495       iamtheonlyjosie  Ahhh... back in a *real* text editing environm...  \n",
       "496             plutopup7  Trouble in Iran, I see. Hmm. Iran. Iran so far...  \n",
       "497          captain_pete  Reading the tweets coming out of Iran... The w...  \n",
       "\n",
       "[498 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e84696",
   "metadata": {},
   "source": [
    "Next, it is necessary to remove columns that will not be useful for training the model and verify if the columns have values in the appropriate format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a1f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(['The id of the tweet', 'The date of the tweet', 'The query', 'The user that tweeted'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5e9eabd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>The text of the tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment                              The text of the tweet\n",
       "0            4  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "1            4  Reading my kindle2...  Love it... Lee childs i...\n",
       "2            4  Ok, first assesment of the #kindle2 ...it fuck...\n",
       "3            4  @kenburbary You'll love your Kindle2. I've had...\n",
       "4            4  @mikefish  Fair enough. But i have the Kindle2...\n",
       "..         ...                                                ...\n",
       "493          2  Ask Programming: LaTeX or InDesign?: submitted...\n",
       "494          0  On that note, I hate Word. I hate Pages. I hat...\n",
       "495          4  Ahhh... back in a *real* text editing environm...\n",
       "496          0  Trouble in Iran, I see. Hmm. Iran. Iran so far...\n",
       "497          0  Reading the tweets coming out of Iran... The w...\n",
       "\n",
       "[498 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd3a5cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 498 entries, 0 to 497\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   Sentiment              498 non-null    int64 \n",
      " 1   The text of the tweet  498 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392f931",
   "metadata": {},
   "source": [
    "After cleaning the data, it is necessary to prepare the bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb3ba847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e70f528c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e0061",
   "metadata": {},
   "source": [
    "When analyzing the sentiment of tweets (positive/neutral/negative), it is also important to consider words such as \"not\" and \"no\". Therefore, these words are excluded from the list of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6efd4f4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, 498):\n",
    "  tweet = re.sub('[^a-zA-Z]', ' ', dataset['The text of the tweet'][i])\n",
    "  tweet = tweet.lower()\n",
    "  tweet = tweet.split()\n",
    "  ps = PorterStemmer()\n",
    "  all_stopwords = stopwords.words('english')\n",
    "  all_stopwords.remove('not')\n",
    "  all_stopwords.remove('no')\n",
    "  tweet = [ps.stem(word) for word in tweet if not word in set(all_stopwords)]\n",
    "  tweet = ' '.join(tweet)\n",
    "  corpus.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3fc653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stellargirl loooooooovvvvvvee kindl not dx cool fantast right', 'read kindl love lee child good read', 'ok first asses kindl fuck rock', 'kenburbari love kindl mine month never look back new big one huge no need remors', 'mikefish fair enough kindl think perfect', 'richardebak no big quit happi kindl', 'fuck economi hate aig non loan given ass', 'jqueri new best friend', 'love twitter', 'not love obama make joke', 'check video presid obama white hous correspond dinner http bit ly imxum', 'karoli firmli believ obama pelosi zero desir civil charad slogan want destroy conservat', 'hous correspond dinner last night whoopi barbara amp sherri went obama got stand ovat', 'watchin espn ju seen new nike commer puppet lebron sh hilari lmao', 'dear nike stop flywir shit wast scienc ugli love vincentx x', 'lebron best athlet gener not time basketbal relat want get inter sport debat', 'talk guy last night tell die hard spur fan also told hate lebron jame', 'love lebron http bit ly pdhur', 'ludajuic lebron beast still cheer til end', 'pmillzz lebron boss', 'sketchbug lebron hometown hero lol love laker let go cav lol', 'lebron zydruna awesom duo', 'wordwhizkid lebron beast nobodi nba come even close', 'download app iphon much fun liter app anyth', 'good news call visa offic say everyth fine relief sick scam steal', 'http twurl nl epkr b awesom come back biz via fredwilson', 'montreal long weekend r amp r much need', 'booz allen hamilton bad ass homegrown social collabor platform way cool ttiv', 'mluc custom innov award winner booz allen hamilton http ping fm c hpp', 'sochi current use nikon love not much canon chose video featur mistak', 'need suggest good ir filter canon got pl dm', 'surfit check googl busi blip show second entri huh good ba http blip fm emhv', 'phyreman googl alway good place look mention work mustang w dad kimblet', 'play android googl phone slide screen scare would break fucker fast still prefer iphon', 'us plan resum militari tribun guantanamo bay time trial aig exec chrysler debt holder', 'omg bore amp tattoooo itchi help aha', 'itchi miser', 'sekseemess no not itchi mayb later lol', 'rt jessverr love nerdi stanford human biolog video make miss school http bit ly nr', 'spinuzzi bit crazi steep learn curv lyx realli good long doc anyth shorter would insan', 'listen p danni gokey lt lt lt aww amaz lt much', 'go sleep bike ride', 'cant sleep tooth ach', 'blah blah blah old old no plan today go back sleep guess', 'glad didnt bay breaker today freak degre san francisco wtf', 'san francisco bay breaker', 'land san francisco', 'san francisco today suggest', 'obama administr must stop bonus aig ponzi schemer http bit ly cuig', 'start think citi realli deep amp gonna surviv turmoil gonna next aig', 'shaunwoo hate n aig', 'yarnth not regret go see star trek awesom', 'way see star trek esquir', 'go see star trek soon dad', 'annoy new trend internet peopl pick apart michael lewi malcolm gladwel nobodi want read', 'bill simmon convers malcolm gladwel http bit ly j', 'highli recommend http tinyurl com howdavidbeatsgoliath malcolm gladwel', 'blink malcolm gladwel amaz book tip point', 'malcolm gladwel might new man crush', 'omg commerci alon espn go drive nut', 'robmalon play twitter api sound fun may need take class find new friend like gener result api code', 'play curl twitter api', 'hello twitter api', 'play java twitter api', 'morind twitter api slow client good', 'yahoo answer butt sometim', 'scrapbook nic', 'rt mashabl five thing wolfram alpha better vastli differ googl http bit ly nsnr', 'chang default pic nike basketbal caus bball awesom', 'nike own nba playoff ad w lebron kobe carmelo http ow ly uiy adida billup howard market brand', 'next time call nike', 'new blog post nike sb dunk low premium white gum http tr im lott', 'rt smartchickpdx told nike layoff start today', 'back work nike one fav word', 'way total inspir freaki nike commerci http snurl com icgj', 'give weka app engin interfac use bird strike data test logo given', 'brand new canon eo mp dslr camera canon mm len web technolog thread brand new canon eo http u mavrev com', 'class suppos come today', 'need someon explain lambda calculu', 'took graduat field exam comput scienc today noth make feel like idiot lambda calculu', 'shout out east palo alto buildin karizmakaz cal gta also thank profit doom univers hempz cracka', 'legalgeekeri yeahhhhhhhhh realli live east palo alto could avoid guess summer', 'accanni edog great stanford cours thank make avail public realli help inform start', 'nvidia name stanford bill dalli chief scientist vp research http bit ly fvvg', 'new blog post harvard versu stanford win http bit ly mcoco', 'work til pm let go laker', 'damn north korea http bit ly ktmeq', 'go ahead blow north korea map alreadi', 'north korea pleas ceas douchebaggeri china even like anymor http bit ly nehsl', 'hell pelosi freakin china whose dime', 'burn cash chrysler gm stop financi tsunami bailout mean take handout', 'insect infect spinach plant', 'wish could catch everi mosquito world n burn em slowli bitin shit outta day mosquito asshol insect', 'got back church total hate insect', 'got mcdonald goddam egg make sick yeah laker date go laker not much updat well true suck', 'omgg ohhde want mcdonald damn wonder open lol', 'histori exam studi ugh', 'hate revis bore total unprepar exam tomorrow thing not look good', 'higher physic exam tommorow not lookin forward much', 'bank holiday yet work exam season suck', 'cheney bush real culprit http fwix com articl', 'life bitch dick cheney p bipart tlot tcot hhr gop dnc http gd djyq', 'dick cheney dishonest speech tortur terror obama fred kaplan slate http gd dihg', 'republican parti bunch anti abort zealot draw fli dump neal boortz radio', 'twitter connect api broken tweet make twitter', 'srsli hate stupid twitter api timeout thing soooo annoy', 'psychemedia realli like kswedberg learn jqueri book http bit ly pg lt worth look', 'jqueri ui book review http cfblogger org c', 'interest ad adob goodbi silverstein amp partner youtub adob cs le sen propr http bit ly vprpt', 'goodbi silverstein agenc new site http www goodbysilverstein com great', 'rt designplay goodbi silverstein new site http www goodbysilverstein com enjoy nice find', 'ever amaz psyop goodbi silverstein amp partner hp http bit ly g ru go play effect', 'top ten watch viral video chart love nike mostvaluablepuppet campaign wieden amp kennedi http bit ly nr n', 'zomg g', 'ok lot buzz io lucki free g http gd hyzl', 'got free g android googl', 'guess retir g start use develop g woot googleio', 'gwt firesid chat googleio', 'happi philip googleio today', 'laker play great cannot wait thursday night laker vs', 'hi anyon great sourc advic viral market http link gs ytz', 'judd apatow creat fake sitcom nbc com market new movi viral market best http gd k yk', 'case studi use viral market add peopl list http snipr com oz', 'viral market fail acia pill brand oughta get shut hack peopl messeng get msg day arrrgh', 'watch night museum lmao', 'love night museum', 'go see new night museum movi famili oh boy three year old movi fuin', 'got back movi went see new night museum rachel good', 'saw new night museum movi okay lol', 'go see night museum tall boy', 'shannyoday take date see night museum whenev want look soooooo good', 'no watch night museum get realli good', 'night museum wolverin junk food perfect monday', 'saw night museum last night pretti crazi movi cast awesom well worth robin william forev', 'saw night museum battl swithsonian today okay typic kid ben stiller movi', 'take kati see night museum pick', 'night museum tonit instead oh well yr old better enjoy lol', 'gm say expect announc sale hummer soon reuter wdsugm say expect announc sale hummer http bit ly e fv', 'unfortun stimulu plan put place twice help gm back american peopl led inevit', 'tell give gm use program support unemploy', 'jdreiss oh ye gm die worth boo hahaha', 'time warner cabl rd time sinc memori day bummer', 'would rather pay reason yearli tax free fast internet get goug time warner slow connect', 'nooooooo dvr die half way ea presser hate time warner', 'f ck time warner cabl f cking suck ball hd tv amp damn hd channel hardli ever come bullshit', 'time warner wors custom servic ever never use', 'time warner devil worst possibl time internet go', 'fuck no internet damn time warner', 'time warner realli pick worst time not work want get mtv com watch hill wtfffff', 'hate time warner soooo wish vio cant watch fricken met game w buffer feel like im watch free internet porn', 'ahh got rid stupid time warner today amp take nap roomi cook pretti good end monday', 'time warner hd line crap', 'fuck time warner cabl didnt know modem could explod susan boyl suck', 'time warner cabl pull plug girlfriend experi www tinyurl com fk', 'time warner cabl slogan call day pm happen', 'rocawear head china build store http tinyurl com nofet', 'climat focu turn beij unit nation us european govern call china co http tinyurl com lto n', 'myfoxdc barri student back trip china silver spring high school class trip china en http tinyurl com nlhqba', 'three china aerospac giant develop tianjin binhai new area b yuan invest http bit ly mmidv', 'http xi gs fo gm ceo china continu key partner', 'rt latimesauto time buy gm car http bit ly nrzlu', 'recov surgeri wish julesrenn', 'wrist still hurt get look hate dr dentist scari place time watch eagl eye want join txt', 'dentist tomorrow brush well morn like make hair nice get cut', 'dentist lie u feel discomort prob even need pain pill man u twippin shit hurt mani pill take', 'kirstiealley dentist great expens', 'kirstiealley pet dentist http www funnyvil com fv pictur dogdentur shtml', 'stude math tomorrow exam dentist', 'dentist wrong wrong', 'go dentist later', 'son look car onlin hate car shop would rather go dentist anyon good car good price sell', 'ncaa basebal super region ram club http bit ly ro nx', 'start play major leagu basebal k http raptr com h llgwar', 'cardin basebal advanc super region face cs fullerton friday', 'soni coupon code expir soon http www coupondork com r', 'wait line safeway', 'luke got stop walk safeway ask empti pocket lift shirt jack', 'not realiz gym safeway', 'xphile three word safeway dot com', 'safeway rock n roll tonight', 'bout hit safeway gotta eat', 'jake go safeway', 'found safeway pick stapl', 'safeway super market via mobil coupon http bit ly onh w', 'safeway bathroom still smell like ass', 'safeway elkhorn move like dead', 'normal weight get normal eat blog http bit ly zet', 'eat watch movi', 'eat sashimi', 'eat home made yema', 'eat cake', 'love dwight howard vitamin water commerci wish nike not adida lol', 'found noth nike factori banana republ outlet http myloc zic', 'iphon may get radio tag nike recent releas itun version suggest voiceov function http tinyurl com oq ctc', 'lovin nike alreadi run spot bedroom', 'launch http imgsearch net imgsearch ajax jqueri webapp', 'matthewcyan final got around use jqueri make bio collaps yay slide anim', 'rt jqueri ultim jqueri list http jquerylist com', 'extract open sourc jqueri plugin stormweight highlight text regular express http bit ly ybjkb', 'anna debenham php jqueri hack', 'jqueri cheat sheet http www javascripttoolbox com jqueri cheatsheet', 'begin javascript css develop jqueri javascript css jqueri http bit ly e', 'pdubyad right lol get high expect warren buffet style', 'rt blknpreciou rt great dbroo someon sit shade today someon plant tree long time ago warren buffet', 'warren buffet economi http ping fm lau p', 'warren buffet becam time richest man unit state not work invest big idea lead fortun', 'accord creat school notr dame receiv ncaa higher rate sweet', 'star basketbal classic tuesday featur top talent chattanooga notr dame high school play host http bit ly qltja', 'blondebroad definit warranti amp experi amazon support kindl great contact kindl', 'rt look avail amazon kindl amp kindl dx get http short ub top electron book reader period free day ship', 'time warner road runner custom support absolut blow hate not high speed net option readi go nuclear', 'time warner cabl phone rep r dumber nail ugh cabl work min ago not wtf', 'siratomofbon tri time warner nice record today', 'omg time warner f ed internet instal instead today next saturday anoth week w internet amp ehfa v fhg fml', 'wth never seen line loooong time warner ugh', 'impati await arriv time warner guy way pretti insid afternoon', 'man accost roger feder french open http ff im hcpt', 'naiv bay use em text classif realli frustrat', 'went stanford univers today got tour made want go back colleg also decid kid go', 'investig pend death stanford cs prof googl mentor rajeev motwani http bit ly lwour tip techmem', 'go bed success weekend stanford come', 'karrisfoxi harass call car warranti chang number fix call everi number bag', 'block unit blood servic use googl voic call car warranti guy', 'amp complet fail', 'broskiii oh snap work amp', 'mbjthegreat realli dont want amp phone servic suck come signal', 'say cut small talk amp new slogan f k give us money apolog bob geldof', 'piss amp mid contract upgrad price iphon not go pay someth thought', 'safari fast even shitti amp tether', 'im amp fuck', 'springsingfiend dvyer sethdaggett jlshack amp drop ball support crap new iphon fail att suck', 'mmbarnhil yay glad got phone still damn amp', 'googl wave develop sandbox account request http bit ly nylc', 'talk cheap bing stick googl http bit ly xc c', 'defsound wtf point delet tweet still found summiz search twitter pleas fix thank bye', 'mattcutt googl profil stop show search cant see anymor', 'arunbasil love googl translat good day mate', 'read new kindl', 'kindl came love', 'love new kindl name kendra case u wonder cookbook tool cuz tell u trick best gift evr', 'real aig scandal http bit ly b px', 'twitter apr app yet', 'pro follow twitter http gd smbz', 'obama quit good comedian check dinner speech cnn funni joke', 'barack obama show funni side gt gt http tr im l gy great speech', 'like guy barack obama show funni side gt gt http tr im l gy', 'obama speech pretti awesom last night http bit ly imxum', 'read bill clinton fail obama win http tinyurl com pcyxj', 'obama popular u among arab survey presid barack obama popular lead arab countri http tinyurl com prlvqu', 'obama got joke haha got watch bit dinner speech last night love mr presid', 'lebron jame got car accid guess heard even news wow cant believ ok http twtad com', 'best playoff year oh yea lebron melo final', 'khalid no lebron best', 'real usher lebron cool like person good charact', 'watch lebron highlight damn nigga good', 'lou lebron murder shit', 'uscsport lebron monsta smh world readi', 'cthagod lebron done nba probabl greater kobe like u said kobe good alot good player', 'kobe good bt lebron vote', 'kobe best world not lebron', 'asherroth world cup access damn good look', 'bought ticket fifa world cup south africa go great summer http bit ly gezi', 'share disrupt fred wilson slide talk googl hq http bit ly bo pg', 'go booz allen hamilton hr meet get go home', 'great indian tamasha truli unfold may result day indian gener elect', 'crlane kindl seen pictur dx seen person love kindl everyday', 'criticalpath awesom idea continu learn program kindl http bit ly zlff', 'ok noth think', 'faithbabywear ooooh model get love love love love', 'time india wonder india elect http bit ly p u h', 'http gd aruj good video googl use search option', 'ambcharlesfield lol ah skin itchi damn lawnmow', 'itchi back dont ya hate', 'stanford chariti fashion show top draw http cli gs nenuah', 'stanford univers facebook profil one popular offici univers page http tinyurl com p b fl', 'lyx cool', 'sooo dissapoint sent danni gokey home still rock danni hometown hero yeah milrocke', 'rt passionmodel american idol fashion adam lambert tone danni gokey cute http cli gs jwshv', 'dannygokey love danni gokey', 'rt justindavey rt tweetmem gm onstar instantli send accid locat coordin gp obsess http bit ly szl', 'tire sleep well last night', 'board plane san francisco hour hr flight blech', 'bonjour san francisco back hurt last night', 'breaker san francisco ca http loopt us v bw', 'head san francisco', 'best girl hour san francisco mmmmmfamili wonder', 'f big go home aig', 'went see star trek movi last night satisfi', 'wait go see star trek tonight', 'star trek good everyon said', 'love new malcolm gladwel book outlier', 'highli recommend malcolm gladwel tip point next audiobook probabl one well', 'malcolm gladwel geniu trick peopl not realiz fuck idiot', 'sportsguy hey no offens malcolm gladwel preteni annoy cunt bring cant read shit', 'rt clashmor http bit ly soyv great articl malcolm gladwel', 'serious underestim malcolm gladwel want meet dude', 'hate comcast right everyth cabl internet amp phone ughh', 'comcast suck', 'day never deal comcast rank one best day life', 'dommm comcast fail', 'use twitter api http bit ly vbhh', 'curs twitter api limit', 'see dave winer scream lack twitter api limit access throttl', 'test twitter api', 'arg twitter api make crazi', 'test twitter api remot updat', 'realli love new search site wolfram alpha make googl seem quaint http www wolframalpha com', 'wolfram alpha suck even research inform provid less get googl wikipedia total useless', 'nike factori', 'new nike muppet commerci pretti cute live togeth', 'new blog post nike zoom lebron soldier iii white black teal http bit ly rouu', 'new blog post nike trainer http bit ly bp', 'fraggl oh awesom wish own nike', 'tonyhawk http twitpic com c uj awesom see show friday shorelin amphitheatr never seen nin wait', 'arhh weka bug spent almost two hour find crappi', 'mitz hey bud np love although love mkii', 'jonduena robynlyn got us offic', 'pick new canon beauti prepar serious awesom photographi', 'got new toy canon love love love', 'learn lambda calculu', 'job sitterc help take care sick child east palo alto ca http tinyurl com qwrr', 'move east palo alto', 'atebit finish watch stanford iphon class session realli appreci rock', 'jktweet hi saw stanford talk realli like advic say hi singapor ye video get around', 'mba admiss tip stanford gsb deadlin essay topic http tinyurl com pet fd', 'ethic nonprofit http bit ly qsxrp stanford socialentrepreneurship', 'laker tonight let go', 'laker kick nugget ass tonight', 'oooooooh north korea troubleeee http bit ly epah', 'wat heck north korea conduct power nuclear test follow link http www msnbc msn com id', 'listen obama friggin north korea', 'realiz three monkey white obama biden pelosi sarah palin', 'foxnew pelosi stay china never come back', 'nanci pelosi gave worst commenc speech ever heard ye still bitter', 'ugh amount time stupid insect bitten grr', 'prettiest insect ever pink katydid http bit ly upw p', 'got barrag hord insect hungri kitchen light scari', 'mcdonald dinner goooood big mac meal', 'ahh ye lol ima tell hubbi go get sum mcdonald', 'stop lunch mcdonald chicken nuggetssss yummmmmi', 'could go lot mcdonald mean lot', 'exam went good helloleoni prayer work', 'one exam left happi', 'math review im go fail exam', 'colin powel rock yesterday cb cheney need shut hell go home powel man honor serv countri proudli', 'obvious not side cheney http bit ly j', 'absolut hilari mashabl http bit ly bccwt', 'mashabl never thank includ top twitter author rock amp new wave http bit ly eorfv', 'learn jqueri book review http cfblogger org c', 'rt shrop awesom jqueri refer book coda http www macpeep com coda webdesign', 'send e mail like crazi today contact anyon contact goodbi silverstein love speak', 'adob cs commerci goodbi silverstein http bit ly aikhf', 'goodbi silverstein new site http www goodbysilverstein com enjoy', 'wow everyon googl confer got free g month unlimit servic', 'vkerkez dood got free googl android phone confer g', 'orli g amaz btw huge improv g', 'html demo lot great stuff come ye excit http htmlfive appspot com io googleio', 'googleio http twitpic com shi yay happi place place place love googl', 'googleio bring graphic browser nice tbh funfun', 'awesom viral market funni peopl http www nbc com yo teach', 'watch programm life hitler enhanc geeki histori', 'saw night museum sheer desper fund movi', 'night museum pretti furkin good', 'watch night museum giggl', 'pambeeslyjenna jenna went see night museum today surpris see three cast member offic', 'watch night museum ryan staci', 'get readi go watch night museum dum dum give gum gum', 'back see star trek night museum star trek amaz night museum eh', 'watch night museum stinkin cute', 'night museum awesom much better part next weekend see', 'think may new favorit restaur way see night museum', 'sold see night museum year old', 'saw new night museum love next go see', 'shame gm forc make car white hous think sell think', 'u may notic not happi gm situat aig lehman et al', 'obama nation gm short term ap http tinyurl com md r', 'pittstock gm good riddanc sad though', 'never buy govern motor vehicl recent drove gm car sinc bought http tinyurl com lulsw', 'old coca cola guy gm board stupid heck tcot ala', 'rantsandrav worst thing gm concord pleasant hill martinez fuck uaw http buzzup com ueb', 'give man fish u feed day teach fish u feed life buy gm u f k good', 'hear gm thing angri get billion wast bullshit someth like k employe', 'quanttrad gm car junk far qualiti compar honda', 'sad day bankrupt gm', 'upset whole gm thing life know screw', 'whoever run time warner need repeatedli rape rhino understand consequ put shitti cabl svc', 'time warner ceo hint onlin fee magazin ap read mountain view unit state view http bit ly udfch', 'wftb join bit late connect boo time warner', 'cox time warner cox cheaper get b dslreport tw expens get c', 'furiou time warner phone promot', 'got home chick fil boy damn internet stupid time warner', 'could time warner cabl suck no', 'piss time warner causin slow internet problem', 'sportsguy ummm time warner problem', 'guy see time warner suck much ass realli wish could get u vers apart http bit ly j', 'rt sportsguy upsid time warner unhelp phone oper superslow site servic crap not upsid', 'rt sportsguy new time warner slogan time warner make long day cabl', 'confirm time warner fault not facebook fb take minut load tempt switch verizon', 'sportsguy time warner epic fail', 'lawson head newedg hong kong http bit ly xlqsd busi china', 'weird piano guitar hous china http u', 'send us gm chevi photo http tinyurl com luzkpq', 'know sad rt caseymerci st day hurrican season less scarey govt take gm', 'gm file bankruptci not good sign', 'yanke met lost good day', 'dentist appt today actual quit enjoy', 'hate ef dentist', 'stevemoakl dentist appt morn convers', 'kirstiealley hate go dentist', 'hate dentist invent anyway', 'dentist offic cold', 'check video david dentist http bit ly aw', 'first dentist appoint year wednesday possibl', 'tom shanahan latest column sdsu ncaa basebal region appear http ow ly axhu', 'baseballamerica com blog basebal america prospect blog blog http bit ly ett', 'portland citi polit may undo basebal park http tinyurl com lpjquj', 'rt watersisweb ca merc water bottl safeway resold profit well dri across counti http tinyurl com mb', 'drop broccoli walk home safeway depress', 'ronjon safeway', 'appli safeway yeeeee', 'safeway place nightmar right bum', 'safeway dad', 'hate safeway select green tea icecream bought two carton wast money gt lt', 'safeway marvin janel aunti lhu', 'safeway offer mobil coupon http bit ly onh w', 'philli drive cadillac top cali win http tinyurl com nzcjqa', 'save money opt groceri store trip stock food hotel room fridg vs eat everi night town', 'loung around eat taco bell watch nci work tonight need help stay awak', 'eat breakfast school', 'still hungri eat', 'tip healthi eat resultsbi fit blog fit http bit ly gfn', 'boyfriend eat quesadilla', 'eat dinner meat chip risotto', 'got new pair nike shoe pic later', 'nike sb blazer high acg custom brad dougla http timesurl', 'nike rock super grate done amp european divis nike beyond whitstyl muchasmuert', 'nike air yeezi khaki pink colorway releas http shar es bjfn', 'evelynbyrn tri nike v addict', 'erickoston look aw lot like one nike privat jet sayin', 'nike train club beta iphon app look interest', 'argghhhh jqueri appear safari bad safari', 'devsnippet jqueri tool javascript ui compon web http inblog org go hfuqt', 'ajax jqueri css javascript mani exampl http ajaxian com', 'readi drop pretens forev love jqueri want marri sorri ladi nerd jqueri spokenfor js', 'cold look googl chart visual api found jqueri wrapper api http tinyurl com mq bq', 'spent day read jqueri book start drink delirium tremen', 'jqueri selector http codylindley com jqueryselector', 'implement news ticker jqueri ten line code http bit ly cznfj', 'buffet warren buffett kick butt battl boot post alex cripp http bit ly auizo', 'super investor great weekend read warren buffet oldi goodi http tinyurl com oqxgga', 'truli braindead come warren buffet name save soul', 'read michael palin book python year great book also recommend warren buffet amp nelson mandela bio', 'mean notr dame good school closer dan enjoy', 'watch tv without tivo year time warner dvr still suck http www davehitt com march twdvr html', 'say sport writer idiot say roger feder one best ever tenni roger feder best ever tenni', 'still love kindl read new york time not feel natur miss bloomingdal ad', 'love kindl no stack book trip way loo', 'although today keynot rock everi great announc amp shit us littl bit', 'sheridanmarfil not much obsess cell phone iphon slave amp forev', 'freitasm oh see thought amp mhz wcdma', 'plip read tether support phil amp join', 'fuzzbal fun amp p http fuzz ball com twitter', 'today good day dislik amp vote offic inde danielpunkass', 'got wave sandbox invit extra excit bad class play soon enough io wave', 'look like summiz gone mani tweet wwdc perhap', 'hope girl work buy kindl', 'miss insight fill may column one smart guy look close impress kindl http bit ly pey wroush', 'sklososki thank much one happi kindl winner surpris fabul thank best kathleen', 'man kinda dislik appl right case point iphon gs wish video record app pleas http bit ly dzm', 'cwong kindl amp soni pr like physic devic feel good font nice pg turn snappi enuf ui littl klunki', 'kindl seem best eread work uk get one', 'googl addict thank point annamartin hahaha', 'rubi gem primari debit card visa electron', 'bank get new visa platinum card', 'dearest googl rich bastard visa card sent work screw littl guy like', 'date bobbi flay gut fieri food network', 'excit see bobbi flay guy fieri tomorrow great american food amp music fest', 'gonna go see bobbi flay moro shorelin eat drink gonna good', 'wait great american food music festiv shorelin tomorrow mmm katz pastrami bobbi flay ye pleas', 'dad ny day ate mesa grill last night met bobbi flay much fun except complet lost voic today', 'fight latex', 'iheartseveru love want die latex devil', 'hour hour inkscap crash normal solid rock hour latex complain slightest thing take', 'track iran social media http bit ly boqu', 'shit hit fan iran crazi inde iranelect', 'monday alreadi iran may implod kitchen disast annagoss seem happi sebul nice weekend goldpanda great whoop', 'twitter stock buzz aapl es f spi spx palm updat pm', 'get readi test burger receip weekend bobbi flay great receip tri thank bobbi', 'johncmay bobbi flay join', 'lam love bobbi flay favorit rt terrysimpson bflay need place phoenix great pepper', 'creat first latex file scratch work well see amandabittn great time waster', 'use linux love much nicer window look forward use wysiwyg latex editor', 'use latex lot typeset mathemat look hideou', 'ask program latex indesign submit calcio link comment http tinyurl com myfmf', 'note hate word hate page hate latex said hate latex texn rd come kill', 'ahhh back real text edit environ lt latex', 'troubl iran see hmm iran iran far away flockofseagullsweregeopoliticallycorrect', 'read tweet come iran whole thing terrifi incred sad']\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "089dea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08970267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1871"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a09f4e",
   "metadata": {},
   "source": [
    "The specific language used by Twitter users requires the inclusion of all words in the model. Therefore, the exclusion of rarely used words using the max_features parameter of the CountVectorizer class is not applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6b264",
   "metadata": {},
   "source": [
    "The nature of the analyzed data supports the choice of a classification model. The methodology for finding the most accurate model will be as follows:\n",
    "\n",
    "1. Training classification models,\n",
    "2. Checking the accuracy score for the tested model,\n",
    "3. Verification using cross-validation score to assess the accuracy of the result obtained from the accuracy score analysis and determine the value of the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2decaa11",
   "metadata": {},
   "source": [
    "Splitting into training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc79b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb9cda",
   "metadata": {},
   "source": [
    "LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0371e038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1824a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  6  6]\n",
      " [ 6 20  9]\n",
      " [ 6  5 25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.62"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16712388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.60%\n",
      "Standard Deviation: 6.86%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f}%\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55515a3",
   "metadata": {},
   "source": [
    "KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "453eb6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "909328fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17  6  6]\n",
      " [11 11 13]\n",
      " [ 6 10 20]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17e34ef4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.01%\n",
      "Standard Deviation: 7.15%\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f}%\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42672e35",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6de5ba88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9b76b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  6  8]\n",
      " [ 3 21 11]\n",
      " [ 7  4 25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f0077eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.83%\n",
      "Standard Deviation: 8.40%\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f}%\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c22b3",
   "metadata": {},
   "source": [
    "Kernel Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "400da7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61b3d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  5  6]\n",
      " [11 13 11]\n",
      " [ 7  4 25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce60eb69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.07%\n",
      "Standard Deviation: 9.31%\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f}%\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b076eb3c",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed563bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db2b8d9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19  6  4]\n",
      " [ 8 17 10]\n",
      " [ 3  5 28]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c57effea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.60%\n",
      "Standard Deviation: 8.06%\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f}%\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aacc00",
   "metadata": {},
   "source": [
    "Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b455f1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a96c34b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21  6  2]\n",
      " [ 8 23  4]\n",
      " [ 7  5 24]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.68"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad13449f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.34%\n",
      "Standard Deviation: 6.23%\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f}%\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e0b812",
   "metadata": {},
   "source": [
    "Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f590e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03951fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13  4 12]\n",
      " [ 7 18 10]\n",
      " [ 6  6 24]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6188a9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.56%\n",
      "Standard Deviation: 8.89%\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f}%\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480af07",
   "metadata": {},
   "source": [
    "The last model will be XGBoost. Before training the model, it is necessary to modify the source data by changing the values in the 'Sentiment' column from '0, 2, 4' to '0, 1, 2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "234fdb35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['Sentiment'] = dataset['Sentiment'].replace({0: 0, 2: 1, 4: 2})\n",
    "y = dataset.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9a0e801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective='multi:softprob', predictor=None, ...)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c31714e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  6  5]\n",
      " [ 6 20  9]\n",
      " [ 8  7 21]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.59"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1fcce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.06%\n",
      "Standard Deviation: 6.24%\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracies.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f}%\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773f118",
   "metadata": {},
   "source": [
    "Due to the specific vocabulary used by Twitter users, it is challenging to train a model that can accurately determine the sentiment of a tweet (positive/neutral/negative)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
